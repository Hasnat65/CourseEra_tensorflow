{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Exercise4-Question.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasnat65/CourseEra_tensorflow/blob/master/Copy_of_Exercise4_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UncprnB0ymAE",
        "colab_type": "text"
      },
      "source": [
        "Below is code with a link to a happy or sad dataset which contains 80 images, 40 happy and 40 sad. \n",
        "Create a convolutional neural network that trains to 100% accuracy on these images,  which cancels training upon hitting training accuracy of >.999\n",
        "\n",
        "Hint -- it will work best with 3 convolutional layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vti6p3PxmpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import zipfile\n",
        "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, BatchNormalization\n",
        "\n",
        "\n",
        "DESIRED_ACCURACY = 0.999\n",
        " \n",
        "zip_ref = zipfile.ZipFile(\"/tmp/happy-or-sad.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp/h-or-s\")\n",
        "zip_ref.close()\n",
        "\n",
        "#class myCallback( ) \n",
        "\n",
        "#callbacks = myCallback()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-_YH70MuVcE",
        "colab_type": "code",
        "outputId": "614c89d5-01ca-45fe-c20b-f8bf526a71b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " \n",
        "\n",
        "# Directory with our training human pictures\n",
        " "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYPQmdUDWOYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('acc')>=DESIRED_ACCURACY):\n",
        "            print('\\nDesired Accuracy is met, Stopping training...')\n",
        "            self.model.stop_training = True\n",
        "def train_happy_sad_model():\n",
        "\n",
        "    DESIRED_ACCURACY = 0.999\n",
        "\n",
        "\n",
        " \n",
        " \n",
        " \n",
        "    callbacks = myCallback()\n",
        "\n",
        " \n",
        "    data_generator = ImageDataGenerator( \n",
        "      \n",
        "        rescale=1./255,\n",
        "         \n",
        "        )\n",
        "    train_happy  = os.path.join(\"/tmp/h-or-s\")\n",
        "    traindata = data_generator.flow_from_directory(directory=train_happy,target_size=(150 ,150 ),  batch_size=4,)\n",
        "    num_classes = len(traindata.class_indices)\n",
        "\n",
        "\n",
        " \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "\n",
        "        tf.keras.layers.Dense(200, activation='relu'),\n",
        "\n",
        "        tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "\n",
        "            ])\n",
        "\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])# Your Code Here #\n",
        "\n",
        "# This code block should create an instance of an ImageDataGenerator called train_datagen\n",
        "\n",
        "# And a train_generator by calling train_datagen.flow_from_directory\n",
        "\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale=1/255.0)# Your Code Here\n",
        "\n",
        "\n",
        "    history = model.fit_generator(traindata,steps_per_epoch=10,epochs=30,verbose=1,callbacks=[callbacks])\n",
        "    return history.history['acc'][-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACuN8Qz6DZAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "d4846d93-e398-43d9-c8ee-a46eb4979f97"
      },
      "source": [
        "train_happy_sad_model()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80 images belonging to 2 classes.\n",
            "Epoch 1/30\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.8434 - acc: 0.4500\n",
            "Epoch 2/30\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6741 - acc: 0.6375\n",
            "Epoch 3/30\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.5144 - acc: 0.6750\n",
            "Epoch 4/30\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2306 - acc: 0.9000\n",
            "Epoch 5/30\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.4222 - acc: 0.8375\n",
            "Epoch 6/30\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1587 - acc: 0.9750\n",
            "Epoch 7/30\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.2278 - acc: 0.9125\n",
            "Epoch 8/30\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.1439 - acc: 0.9750\n",
            "Epoch 9/30\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1259 - acc: 0.9375\n",
            "Epoch 10/30\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2858 - acc: 0.9250\n",
            "Epoch 11/30\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.2054 - acc: 0.9625\n",
            "Epoch 12/30\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.0624 - acc: 0.9750\n",
            "Epoch 13/30\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.0692 - acc: 0.9750\n",
            "Epoch 14/30\n",
            " 7/10 [====================>.........] - ETA: 0s - loss: 0.0054 - acc: 1.0000\n",
            "Desired Accuracy is met, Stopping training...\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0038 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaBfv4NqL-mD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # GRADED FUNCTION: house_model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "def house_model(y_new):\n",
        "    model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "    model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "    xs = np.array([1.0,2.0,3.0,4.0,5.0,6.0], dtype=float)\n",
        "\n",
        "    ys = np.array([1.0,1.5,2.0,2.5,3.0,3.5], dtype=float)\n",
        "\n",
        "    model.fit(xs,ys, epochs=300)\n",
        "    return model.predict(y_new)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq0dICrhNH0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "274c97a8-190d-41ed-a57f-a6ed73d4dbbe"
      },
      "source": [
        "prediction = house_model([7.0])\n",
        "print(prediction)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6 samples\n",
            "Epoch 1/300\n",
            "6/6 [==============================] - 0s 31ms/sample - loss: 7.3816\n",
            "Epoch 2/300\n",
            "6/6 [==============================] - 0s 681us/sample - loss: 3.4286\n",
            "Epoch 3/300\n",
            "6/6 [==============================] - 0s 556us/sample - loss: 1.5990\n",
            "Epoch 4/300\n",
            "6/6 [==============================] - 0s 648us/sample - loss: 0.7521\n",
            "Epoch 5/300\n",
            "6/6 [==============================] - 0s 600us/sample - loss: 0.3600\n",
            "Epoch 6/300\n",
            "6/6 [==============================] - 0s 590us/sample - loss: 0.1784\n",
            "Epoch 7/300\n",
            "6/6 [==============================] - 0s 496us/sample - loss: 0.0943\n",
            "Epoch 8/300\n",
            "6/6 [==============================] - 0s 458us/sample - loss: 0.0553\n",
            "Epoch 9/300\n",
            "6/6 [==============================] - 0s 334us/sample - loss: 0.0372\n",
            "Epoch 10/300\n",
            "6/6 [==============================] - 0s 292us/sample - loss: 0.0287\n",
            "Epoch 11/300\n",
            "6/6 [==============================] - 0s 343us/sample - loss: 0.0247\n",
            "Epoch 12/300\n",
            "6/6 [==============================] - 0s 339us/sample - loss: 0.0228\n",
            "Epoch 13/300\n",
            "6/6 [==============================] - 0s 344us/sample - loss: 0.0218\n",
            "Epoch 14/300\n",
            "6/6 [==============================] - 0s 301us/sample - loss: 0.0212\n",
            "Epoch 15/300\n",
            "6/6 [==============================] - 0s 320us/sample - loss: 0.0209\n",
            "Epoch 16/300\n",
            "6/6 [==============================] - 0s 387us/sample - loss: 0.0207\n",
            "Epoch 17/300\n",
            "6/6 [==============================] - 0s 310us/sample - loss: 0.0205\n",
            "Epoch 18/300\n",
            "6/6 [==============================] - 0s 358us/sample - loss: 0.0203\n",
            "Epoch 19/300\n",
            "6/6 [==============================] - 0s 292us/sample - loss: 0.0202\n",
            "Epoch 20/300\n",
            "6/6 [==============================] - 0s 292us/sample - loss: 0.0200\n",
            "Epoch 21/300\n",
            "6/6 [==============================] - 0s 803us/sample - loss: 0.0199\n",
            "Epoch 22/300\n",
            "6/6 [==============================] - 0s 483us/sample - loss: 0.0197\n",
            "Epoch 23/300\n",
            "6/6 [==============================] - 0s 458us/sample - loss: 0.0196\n",
            "Epoch 24/300\n",
            "6/6 [==============================] - 0s 475us/sample - loss: 0.0194\n",
            "Epoch 25/300\n",
            "6/6 [==============================] - 0s 611us/sample - loss: 0.0193\n",
            "Epoch 26/300\n",
            "6/6 [==============================] - 0s 723us/sample - loss: 0.0192\n",
            "Epoch 27/300\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0190\n",
            "Epoch 28/300\n",
            "6/6 [==============================] - 0s 919us/sample - loss: 0.0189\n",
            "Epoch 29/300\n",
            "6/6 [==============================] - 0s 619us/sample - loss: 0.0187\n",
            "Epoch 30/300\n",
            "6/6 [==============================] - 0s 506us/sample - loss: 0.0186\n",
            "Epoch 31/300\n",
            "6/6 [==============================] - 0s 805us/sample - loss: 0.0185\n",
            "Epoch 32/300\n",
            "6/6 [==============================] - 0s 382us/sample - loss: 0.0183\n",
            "Epoch 33/300\n",
            "6/6 [==============================] - 0s 476us/sample - loss: 0.0182\n",
            "Epoch 34/300\n",
            "6/6 [==============================] - 0s 329us/sample - loss: 0.0181\n",
            "Epoch 35/300\n",
            "6/6 [==============================] - 0s 367us/sample - loss: 0.0179\n",
            "Epoch 36/300\n",
            "6/6 [==============================] - 0s 413us/sample - loss: 0.0178\n",
            "Epoch 37/300\n",
            "6/6 [==============================] - 0s 380us/sample - loss: 0.0177\n",
            "Epoch 38/300\n",
            "6/6 [==============================] - 0s 405us/sample - loss: 0.0175\n",
            "Epoch 39/300\n",
            "6/6 [==============================] - 0s 302us/sample - loss: 0.0174\n",
            "Epoch 40/300\n",
            "6/6 [==============================] - 0s 672us/sample - loss: 0.0173\n",
            "Epoch 41/300\n",
            "6/6 [==============================] - 0s 325us/sample - loss: 0.0172\n",
            "Epoch 42/300\n",
            "6/6 [==============================] - 0s 376us/sample - loss: 0.0170\n",
            "Epoch 43/300\n",
            "6/6 [==============================] - 0s 369us/sample - loss: 0.0169\n",
            "Epoch 44/300\n",
            "6/6 [==============================] - 0s 782us/sample - loss: 0.0168\n",
            "Epoch 45/300\n",
            "6/6 [==============================] - 0s 435us/sample - loss: 0.0167\n",
            "Epoch 46/300\n",
            "6/6 [==============================] - 0s 450us/sample - loss: 0.0165\n",
            "Epoch 47/300\n",
            "6/6 [==============================] - 0s 354us/sample - loss: 0.0164\n",
            "Epoch 48/300\n",
            "6/6 [==============================] - 0s 439us/sample - loss: 0.0163\n",
            "Epoch 49/300\n",
            "6/6 [==============================] - 0s 271us/sample - loss: 0.0162\n",
            "Epoch 50/300\n",
            "6/6 [==============================] - 0s 486us/sample - loss: 0.0161\n",
            "Epoch 51/300\n",
            "6/6 [==============================] - 0s 328us/sample - loss: 0.0160\n",
            "Epoch 52/300\n",
            "6/6 [==============================] - 0s 537us/sample - loss: 0.0158\n",
            "Epoch 53/300\n",
            "6/6 [==============================] - 0s 332us/sample - loss: 0.0157\n",
            "Epoch 54/300\n",
            "6/6 [==============================] - 0s 471us/sample - loss: 0.0156\n",
            "Epoch 55/300\n",
            "6/6 [==============================] - 0s 361us/sample - loss: 0.0155\n",
            "Epoch 56/300\n",
            "6/6 [==============================] - 0s 328us/sample - loss: 0.0154\n",
            "Epoch 57/300\n",
            "6/6 [==============================] - 0s 423us/sample - loss: 0.0153\n",
            "Epoch 58/300\n",
            "6/6 [==============================] - 0s 284us/sample - loss: 0.0152\n",
            "Epoch 59/300\n",
            "6/6 [==============================] - 0s 269us/sample - loss: 0.0150\n",
            "Epoch 60/300\n",
            "6/6 [==============================] - 0s 404us/sample - loss: 0.0149\n",
            "Epoch 61/300\n",
            "6/6 [==============================] - 0s 532us/sample - loss: 0.0148\n",
            "Epoch 62/300\n",
            "6/6 [==============================] - 0s 498us/sample - loss: 0.0147\n",
            "Epoch 63/300\n",
            "6/6 [==============================] - 0s 800us/sample - loss: 0.0146\n",
            "Epoch 64/300\n",
            "6/6 [==============================] - 0s 411us/sample - loss: 0.0145\n",
            "Epoch 65/300\n",
            "6/6 [==============================] - 0s 489us/sample - loss: 0.0144\n",
            "Epoch 66/300\n",
            "6/6 [==============================] - 0s 733us/sample - loss: 0.0143\n",
            "Epoch 67/300\n",
            "6/6 [==============================] - 0s 496us/sample - loss: 0.0142\n",
            "Epoch 68/300\n",
            "6/6 [==============================] - 0s 435us/sample - loss: 0.0141\n",
            "Epoch 69/300\n",
            "6/6 [==============================] - 0s 308us/sample - loss: 0.0140\n",
            "Epoch 70/300\n",
            "6/6 [==============================] - 0s 397us/sample - loss: 0.0139\n",
            "Epoch 71/300\n",
            "6/6 [==============================] - 0s 401us/sample - loss: 0.0138\n",
            "Epoch 72/300\n",
            "6/6 [==============================] - 0s 495us/sample - loss: 0.0137\n",
            "Epoch 73/300\n",
            "6/6 [==============================] - 0s 699us/sample - loss: 0.0136\n",
            "Epoch 74/300\n",
            "6/6 [==============================] - 0s 426us/sample - loss: 0.0135\n",
            "Epoch 75/300\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0134\n",
            "Epoch 76/300\n",
            "6/6 [==============================] - 0s 661us/sample - loss: 0.0133\n",
            "Epoch 77/300\n",
            "6/6 [==============================] - 0s 386us/sample - loss: 0.0132\n",
            "Epoch 78/300\n",
            "6/6 [==============================] - 0s 606us/sample - loss: 0.0131\n",
            "Epoch 79/300\n",
            "6/6 [==============================] - 0s 411us/sample - loss: 0.0130\n",
            "Epoch 80/300\n",
            "6/6 [==============================] - 0s 315us/sample - loss: 0.0129\n",
            "Epoch 81/300\n",
            "6/6 [==============================] - 0s 419us/sample - loss: 0.0128\n",
            "Epoch 82/300\n",
            "6/6 [==============================] - 0s 413us/sample - loss: 0.0127\n",
            "Epoch 83/300\n",
            "6/6 [==============================] - 0s 681us/sample - loss: 0.0126\n",
            "Epoch 84/300\n",
            "6/6 [==============================] - 0s 438us/sample - loss: 0.0125\n",
            "Epoch 85/300\n",
            "6/6 [==============================] - 0s 513us/sample - loss: 0.0124\n",
            "Epoch 86/300\n",
            "6/6 [==============================] - 0s 441us/sample - loss: 0.0123\n",
            "Epoch 87/300\n",
            "6/6 [==============================] - 0s 591us/sample - loss: 0.0123\n",
            "Epoch 88/300\n",
            "6/6 [==============================] - 0s 328us/sample - loss: 0.0122\n",
            "Epoch 89/300\n",
            "6/6 [==============================] - 0s 407us/sample - loss: 0.0121\n",
            "Epoch 90/300\n",
            "6/6 [==============================] - 0s 569us/sample - loss: 0.0120\n",
            "Epoch 91/300\n",
            "6/6 [==============================] - 0s 365us/sample - loss: 0.0119\n",
            "Epoch 92/300\n",
            "6/6 [==============================] - 0s 408us/sample - loss: 0.0118\n",
            "Epoch 93/300\n",
            "6/6 [==============================] - 0s 373us/sample - loss: 0.0117\n",
            "Epoch 94/300\n",
            "6/6 [==============================] - 0s 425us/sample - loss: 0.0116\n",
            "Epoch 95/300\n",
            "6/6 [==============================] - 0s 459us/sample - loss: 0.0116\n",
            "Epoch 96/300\n",
            "6/6 [==============================] - 0s 323us/sample - loss: 0.0115\n",
            "Epoch 97/300\n",
            "6/6 [==============================] - 0s 464us/sample - loss: 0.0114\n",
            "Epoch 98/300\n",
            "6/6 [==============================] - 0s 348us/sample - loss: 0.0113\n",
            "Epoch 99/300\n",
            "6/6 [==============================] - 0s 397us/sample - loss: 0.0112\n",
            "Epoch 100/300\n",
            "6/6 [==============================] - 0s 384us/sample - loss: 0.0111\n",
            "Epoch 101/300\n",
            "6/6 [==============================] - 0s 415us/sample - loss: 0.0111\n",
            "Epoch 102/300\n",
            "6/6 [==============================] - 0s 491us/sample - loss: 0.0110\n",
            "Epoch 103/300\n",
            "6/6 [==============================] - 0s 315us/sample - loss: 0.0109\n",
            "Epoch 104/300\n",
            "6/6 [==============================] - 0s 322us/sample - loss: 0.0108\n",
            "Epoch 105/300\n",
            "6/6 [==============================] - 0s 516us/sample - loss: 0.0107\n",
            "Epoch 106/300\n",
            "6/6 [==============================] - 0s 305us/sample - loss: 0.0107\n",
            "Epoch 107/300\n",
            "6/6 [==============================] - 0s 352us/sample - loss: 0.0106\n",
            "Epoch 108/300\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0105\n",
            "Epoch 109/300\n",
            "6/6 [==============================] - 0s 341us/sample - loss: 0.0104\n",
            "Epoch 110/300\n",
            "6/6 [==============================] - 0s 451us/sample - loss: 0.0104\n",
            "Epoch 111/300\n",
            "6/6 [==============================] - 0s 452us/sample - loss: 0.0103\n",
            "Epoch 112/300\n",
            "6/6 [==============================] - 0s 543us/sample - loss: 0.0102\n",
            "Epoch 113/300\n",
            "6/6 [==============================] - 0s 435us/sample - loss: 0.0101\n",
            "Epoch 114/300\n",
            "6/6 [==============================] - 0s 440us/sample - loss: 0.0101\n",
            "Epoch 115/300\n",
            "6/6 [==============================] - 0s 383us/sample - loss: 0.0100\n",
            "Epoch 116/300\n",
            "6/6 [==============================] - 0s 353us/sample - loss: 0.0099\n",
            "Epoch 117/300\n",
            "6/6 [==============================] - 0s 464us/sample - loss: 0.0098\n",
            "Epoch 118/300\n",
            "6/6 [==============================] - 0s 408us/sample - loss: 0.0098\n",
            "Epoch 119/300\n",
            "6/6 [==============================] - 0s 394us/sample - loss: 0.0097\n",
            "Epoch 120/300\n",
            "6/6 [==============================] - 0s 513us/sample - loss: 0.0096\n",
            "Epoch 121/300\n",
            "6/6 [==============================] - 0s 459us/sample - loss: 0.0096\n",
            "Epoch 122/300\n",
            "6/6 [==============================] - 0s 387us/sample - loss: 0.0095\n",
            "Epoch 123/300\n",
            "6/6 [==============================] - 0s 706us/sample - loss: 0.0094\n",
            "Epoch 124/300\n",
            "6/6 [==============================] - 0s 349us/sample - loss: 0.0094\n",
            "Epoch 125/300\n",
            "6/6 [==============================] - 0s 488us/sample - loss: 0.0093\n",
            "Epoch 126/300\n",
            "6/6 [==============================] - 0s 567us/sample - loss: 0.0092\n",
            "Epoch 127/300\n",
            "6/6 [==============================] - 0s 581us/sample - loss: 0.0092\n",
            "Epoch 128/300\n",
            "6/6 [==============================] - 0s 493us/sample - loss: 0.0091\n",
            "Epoch 129/300\n",
            "6/6 [==============================] - 0s 437us/sample - loss: 0.0090\n",
            "Epoch 130/300\n",
            "6/6 [==============================] - 0s 418us/sample - loss: 0.0090\n",
            "Epoch 131/300\n",
            "6/6 [==============================] - 0s 330us/sample - loss: 0.0089\n",
            "Epoch 132/300\n",
            "6/6 [==============================] - 0s 322us/sample - loss: 0.0088\n",
            "Epoch 133/300\n",
            "6/6 [==============================] - 0s 534us/sample - loss: 0.0088\n",
            "Epoch 134/300\n",
            "6/6 [==============================] - 0s 504us/sample - loss: 0.0087\n",
            "Epoch 135/300\n",
            "6/6 [==============================] - 0s 338us/sample - loss: 0.0086\n",
            "Epoch 136/300\n",
            "6/6 [==============================] - 0s 328us/sample - loss: 0.0086\n",
            "Epoch 137/300\n",
            "6/6 [==============================] - 0s 379us/sample - loss: 0.0085\n",
            "Epoch 138/300\n",
            "6/6 [==============================] - 0s 571us/sample - loss: 0.0084\n",
            "Epoch 139/300\n",
            "6/6 [==============================] - 0s 604us/sample - loss: 0.0084\n",
            "Epoch 140/300\n",
            "6/6 [==============================] - 0s 675us/sample - loss: 0.0083\n",
            "Epoch 141/300\n",
            "6/6 [==============================] - 0s 705us/sample - loss: 0.0083\n",
            "Epoch 142/300\n",
            "6/6 [==============================] - 0s 657us/sample - loss: 0.0082\n",
            "Epoch 143/300\n",
            "6/6 [==============================] - 0s 849us/sample - loss: 0.0081\n",
            "Epoch 144/300\n",
            "6/6 [==============================] - 0s 657us/sample - loss: 0.0081\n",
            "Epoch 145/300\n",
            "6/6 [==============================] - 0s 745us/sample - loss: 0.0080\n",
            "Epoch 146/300\n",
            "6/6 [==============================] - 0s 777us/sample - loss: 0.0080\n",
            "Epoch 147/300\n",
            "6/6 [==============================] - 0s 964us/sample - loss: 0.0079\n",
            "Epoch 148/300\n",
            "6/6 [==============================] - 0s 693us/sample - loss: 0.0078\n",
            "Epoch 149/300\n",
            "6/6 [==============================] - 0s 859us/sample - loss: 0.0078\n",
            "Epoch 150/300\n",
            "6/6 [==============================] - 0s 818us/sample - loss: 0.0077\n",
            "Epoch 151/300\n",
            "6/6 [==============================] - 0s 909us/sample - loss: 0.0077\n",
            "Epoch 152/300\n",
            "6/6 [==============================] - 0s 861us/sample - loss: 0.0076\n",
            "Epoch 153/300\n",
            "6/6 [==============================] - 0s 794us/sample - loss: 0.0076\n",
            "Epoch 154/300\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0075\n",
            "Epoch 155/300\n",
            "6/6 [==============================] - 0s 782us/sample - loss: 0.0075\n",
            "Epoch 156/300\n",
            "6/6 [==============================] - 0s 909us/sample - loss: 0.0074\n",
            "Epoch 157/300\n",
            "6/6 [==============================] - 0s 884us/sample - loss: 0.0073\n",
            "Epoch 158/300\n",
            "6/6 [==============================] - 0s 840us/sample - loss: 0.0073\n",
            "Epoch 159/300\n",
            "6/6 [==============================] - 0s 963us/sample - loss: 0.0072\n",
            "Epoch 160/300\n",
            "6/6 [==============================] - 0s 882us/sample - loss: 0.0072\n",
            "Epoch 161/300\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0071\n",
            "Epoch 162/300\n",
            "6/6 [==============================] - 0s 964us/sample - loss: 0.0071\n",
            "Epoch 163/300\n",
            "6/6 [==============================] - 0s 956us/sample - loss: 0.0070\n",
            "Epoch 164/300\n",
            "6/6 [==============================] - 0s 911us/sample - loss: 0.0070\n",
            "Epoch 165/300\n",
            "6/6 [==============================] - 0s 850us/sample - loss: 0.0069\n",
            "Epoch 166/300\n",
            "6/6 [==============================] - 0s 716us/sample - loss: 0.0069\n",
            "Epoch 167/300\n",
            "6/6 [==============================] - 0s 859us/sample - loss: 0.0068\n",
            "Epoch 168/300\n",
            "6/6 [==============================] - 0s 853us/sample - loss: 0.0068\n",
            "Epoch 169/300\n",
            "6/6 [==============================] - 0s 669us/sample - loss: 0.0067\n",
            "Epoch 170/300\n",
            "6/6 [==============================] - 0s 717us/sample - loss: 0.0067\n",
            "Epoch 171/300\n",
            "6/6 [==============================] - 0s 672us/sample - loss: 0.0066\n",
            "Epoch 172/300\n",
            "6/6 [==============================] - 0s 682us/sample - loss: 0.0066\n",
            "Epoch 173/300\n",
            "6/6 [==============================] - 0s 779us/sample - loss: 0.0065\n",
            "Epoch 174/300\n",
            "6/6 [==============================] - 0s 813us/sample - loss: 0.0065\n",
            "Epoch 175/300\n",
            "6/6 [==============================] - 0s 806us/sample - loss: 0.0064\n",
            "Epoch 176/300\n",
            "6/6 [==============================] - 0s 991us/sample - loss: 0.0064\n",
            "Epoch 177/300\n",
            "6/6 [==============================] - 0s 432us/sample - loss: 0.0063\n",
            "Epoch 178/300\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0063\n",
            "Epoch 179/300\n",
            "6/6 [==============================] - 0s 992us/sample - loss: 0.0063\n",
            "Epoch 180/300\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0062\n",
            "Epoch 181/300\n",
            "6/6 [==============================] - 0s 986us/sample - loss: 0.0062\n",
            "Epoch 182/300\n",
            "6/6 [==============================] - 0s 520us/sample - loss: 0.0061\n",
            "Epoch 183/300\n",
            "6/6 [==============================] - 0s 512us/sample - loss: 0.0061\n",
            "Epoch 184/300\n",
            "6/6 [==============================] - 0s 966us/sample - loss: 0.0060\n",
            "Epoch 185/300\n",
            "6/6 [==============================] - 0s 601us/sample - loss: 0.0060\n",
            "Epoch 186/300\n",
            "6/6 [==============================] - 0s 573us/sample - loss: 0.0059\n",
            "Epoch 187/300\n",
            "6/6 [==============================] - 0s 713us/sample - loss: 0.0059\n",
            "Epoch 188/300\n",
            "6/6 [==============================] - 0s 515us/sample - loss: 0.0059\n",
            "Epoch 189/300\n",
            "6/6 [==============================] - 0s 606us/sample - loss: 0.0058\n",
            "Epoch 190/300\n",
            "6/6 [==============================] - 0s 651us/sample - loss: 0.0058\n",
            "Epoch 191/300\n",
            "6/6 [==============================] - 0s 546us/sample - loss: 0.0057\n",
            "Epoch 192/300\n",
            "6/6 [==============================] - 0s 622us/sample - loss: 0.0057\n",
            "Epoch 193/300\n",
            "6/6 [==============================] - 0s 507us/sample - loss: 0.0056\n",
            "Epoch 194/300\n",
            "6/6 [==============================] - 0s 565us/sample - loss: 0.0056\n",
            "Epoch 195/300\n",
            "6/6 [==============================] - 0s 593us/sample - loss: 0.0056\n",
            "Epoch 196/300\n",
            "6/6 [==============================] - 0s 618us/sample - loss: 0.0055\n",
            "Epoch 197/300\n",
            "6/6 [==============================] - 0s 524us/sample - loss: 0.0055\n",
            "Epoch 198/300\n",
            "6/6 [==============================] - 0s 563us/sample - loss: 0.0054\n",
            "Epoch 199/300\n",
            "6/6 [==============================] - 0s 407us/sample - loss: 0.0054\n",
            "Epoch 200/300\n",
            "6/6 [==============================] - 0s 728us/sample - loss: 0.0054\n",
            "Epoch 201/300\n",
            "6/6 [==============================] - 0s 648us/sample - loss: 0.0053\n",
            "Epoch 202/300\n",
            "6/6 [==============================] - 0s 722us/sample - loss: 0.0053\n",
            "Epoch 203/300\n",
            "6/6 [==============================] - 0s 626us/sample - loss: 0.0052\n",
            "Epoch 204/300\n",
            "6/6 [==============================] - 0s 648us/sample - loss: 0.0052\n",
            "Epoch 205/300\n",
            "6/6 [==============================] - 0s 535us/sample - loss: 0.0052\n",
            "Epoch 206/300\n",
            "6/6 [==============================] - 0s 611us/sample - loss: 0.0051\n",
            "Epoch 207/300\n",
            "6/6 [==============================] - 0s 574us/sample - loss: 0.0051\n",
            "Epoch 208/300\n",
            "6/6 [==============================] - 0s 562us/sample - loss: 0.0051\n",
            "Epoch 209/300\n",
            "6/6 [==============================] - 0s 622us/sample - loss: 0.0050\n",
            "Epoch 210/300\n",
            "6/6 [==============================] - 0s 630us/sample - loss: 0.0050\n",
            "Epoch 211/300\n",
            "6/6 [==============================] - 0s 537us/sample - loss: 0.0050\n",
            "Epoch 212/300\n",
            "6/6 [==============================] - 0s 829us/sample - loss: 0.0049\n",
            "Epoch 213/300\n",
            "6/6 [==============================] - 0s 696us/sample - loss: 0.0049\n",
            "Epoch 214/300\n",
            "6/6 [==============================] - 0s 689us/sample - loss: 0.0048\n",
            "Epoch 215/300\n",
            "6/6 [==============================] - 0s 584us/sample - loss: 0.0048\n",
            "Epoch 216/300\n",
            "6/6 [==============================] - 0s 1ms/sample - loss: 0.0048\n",
            "Epoch 217/300\n",
            "6/6 [==============================] - 0s 496us/sample - loss: 0.0047\n",
            "Epoch 218/300\n",
            "6/6 [==============================] - 0s 628us/sample - loss: 0.0047\n",
            "Epoch 219/300\n",
            "6/6 [==============================] - 0s 680us/sample - loss: 0.0047\n",
            "Epoch 220/300\n",
            "6/6 [==============================] - 0s 556us/sample - loss: 0.0046\n",
            "Epoch 221/300\n",
            "6/6 [==============================] - 0s 573us/sample - loss: 0.0046\n",
            "Epoch 222/300\n",
            "6/6 [==============================] - 0s 624us/sample - loss: 0.0046\n",
            "Epoch 223/300\n",
            "6/6 [==============================] - 0s 532us/sample - loss: 0.0045\n",
            "Epoch 224/300\n",
            "6/6 [==============================] - 0s 580us/sample - loss: 0.0045\n",
            "Epoch 225/300\n",
            "6/6 [==============================] - 0s 578us/sample - loss: 0.0045\n",
            "Epoch 226/300\n",
            "6/6 [==============================] - 0s 615us/sample - loss: 0.0044\n",
            "Epoch 227/300\n",
            "6/6 [==============================] - 0s 524us/sample - loss: 0.0044\n",
            "Epoch 228/300\n",
            "6/6 [==============================] - 0s 704us/sample - loss: 0.0044\n",
            "Epoch 229/300\n",
            "6/6 [==============================] - 0s 709us/sample - loss: 0.0043\n",
            "Epoch 230/300\n",
            "6/6 [==============================] - 0s 659us/sample - loss: 0.0043\n",
            "Epoch 231/300\n",
            "6/6 [==============================] - 0s 636us/sample - loss: 0.0043\n",
            "Epoch 232/300\n",
            "6/6 [==============================] - 0s 630us/sample - loss: 0.0042\n",
            "Epoch 233/300\n",
            "6/6 [==============================] - 0s 616us/sample - loss: 0.0042\n",
            "Epoch 234/300\n",
            "6/6 [==============================] - 0s 591us/sample - loss: 0.0042\n",
            "Epoch 235/300\n",
            "6/6 [==============================] - 0s 628us/sample - loss: 0.0042\n",
            "Epoch 236/300\n",
            "6/6 [==============================] - 0s 558us/sample - loss: 0.0041\n",
            "Epoch 237/300\n",
            "6/6 [==============================] - 0s 696us/sample - loss: 0.0041\n",
            "Epoch 238/300\n",
            "6/6 [==============================] - 0s 587us/sample - loss: 0.0041\n",
            "Epoch 239/300\n",
            "6/6 [==============================] - 0s 661us/sample - loss: 0.0040\n",
            "Epoch 240/300\n",
            "6/6 [==============================] - 0s 589us/sample - loss: 0.0040\n",
            "Epoch 241/300\n",
            "6/6 [==============================] - 0s 661us/sample - loss: 0.0040\n",
            "Epoch 242/300\n",
            "6/6 [==============================] - 0s 625us/sample - loss: 0.0039\n",
            "Epoch 243/300\n",
            "6/6 [==============================] - 0s 631us/sample - loss: 0.0039\n",
            "Epoch 244/300\n",
            "6/6 [==============================] - 0s 646us/sample - loss: 0.0039\n",
            "Epoch 245/300\n",
            "6/6 [==============================] - 0s 761us/sample - loss: 0.0039\n",
            "Epoch 246/300\n",
            "6/6 [==============================] - 0s 446us/sample - loss: 0.0038\n",
            "Epoch 247/300\n",
            "6/6 [==============================] - 0s 489us/sample - loss: 0.0038\n",
            "Epoch 248/300\n",
            "6/6 [==============================] - 0s 415us/sample - loss: 0.0038\n",
            "Epoch 249/300\n",
            "6/6 [==============================] - 0s 447us/sample - loss: 0.0037\n",
            "Epoch 250/300\n",
            "6/6 [==============================] - 0s 561us/sample - loss: 0.0037\n",
            "Epoch 251/300\n",
            "6/6 [==============================] - 0s 460us/sample - loss: 0.0037\n",
            "Epoch 252/300\n",
            "6/6 [==============================] - 0s 596us/sample - loss: 0.0037\n",
            "Epoch 253/300\n",
            "6/6 [==============================] - 0s 456us/sample - loss: 0.0036\n",
            "Epoch 254/300\n",
            "6/6 [==============================] - 0s 385us/sample - loss: 0.0036\n",
            "Epoch 255/300\n",
            "6/6 [==============================] - 0s 454us/sample - loss: 0.0036\n",
            "Epoch 256/300\n",
            "6/6 [==============================] - 0s 367us/sample - loss: 0.0036\n",
            "Epoch 257/300\n",
            "6/6 [==============================] - 0s 609us/sample - loss: 0.0035\n",
            "Epoch 258/300\n",
            "6/6 [==============================] - 0s 463us/sample - loss: 0.0035\n",
            "Epoch 259/300\n",
            "6/6 [==============================] - 0s 486us/sample - loss: 0.0035\n",
            "Epoch 260/300\n",
            "6/6 [==============================] - 0s 397us/sample - loss: 0.0035\n",
            "Epoch 261/300\n",
            "6/6 [==============================] - 0s 359us/sample - loss: 0.0034\n",
            "Epoch 262/300\n",
            "6/6 [==============================] - 0s 524us/sample - loss: 0.0034\n",
            "Epoch 263/300\n",
            "6/6 [==============================] - 0s 456us/sample - loss: 0.0034\n",
            "Epoch 264/300\n",
            "6/6 [==============================] - 0s 759us/sample - loss: 0.0034\n",
            "Epoch 265/300\n",
            "6/6 [==============================] - 0s 435us/sample - loss: 0.0033\n",
            "Epoch 266/300\n",
            "6/6 [==============================] - 0s 612us/sample - loss: 0.0033\n",
            "Epoch 267/300\n",
            "6/6 [==============================] - 0s 449us/sample - loss: 0.0033\n",
            "Epoch 268/300\n",
            "6/6 [==============================] - 0s 625us/sample - loss: 0.0033\n",
            "Epoch 269/300\n",
            "6/6 [==============================] - 0s 441us/sample - loss: 0.0032\n",
            "Epoch 270/300\n",
            "6/6 [==============================] - 0s 426us/sample - loss: 0.0032\n",
            "Epoch 271/300\n",
            "6/6 [==============================] - 0s 409us/sample - loss: 0.0032\n",
            "Epoch 272/300\n",
            "6/6 [==============================] - 0s 444us/sample - loss: 0.0032\n",
            "Epoch 273/300\n",
            "6/6 [==============================] - 0s 560us/sample - loss: 0.0031\n",
            "Epoch 274/300\n",
            "6/6 [==============================] - 0s 474us/sample - loss: 0.0031\n",
            "Epoch 275/300\n",
            "6/6 [==============================] - 0s 696us/sample - loss: 0.0031\n",
            "Epoch 276/300\n",
            "6/6 [==============================] - 0s 358us/sample - loss: 0.0031\n",
            "Epoch 277/300\n",
            "6/6 [==============================] - 0s 455us/sample - loss: 0.0031\n",
            "Epoch 278/300\n",
            "6/6 [==============================] - 0s 217us/sample - loss: 0.0030\n",
            "Epoch 279/300\n",
            "6/6 [==============================] - 0s 260us/sample - loss: 0.0030\n",
            "Epoch 280/300\n",
            "6/6 [==============================] - 0s 819us/sample - loss: 0.0030\n",
            "Epoch 281/300\n",
            "6/6 [==============================] - 0s 506us/sample - loss: 0.0030\n",
            "Epoch 282/300\n",
            "6/6 [==============================] - 0s 537us/sample - loss: 0.0029\n",
            "Epoch 283/300\n",
            "6/6 [==============================] - 0s 568us/sample - loss: 0.0029\n",
            "Epoch 284/300\n",
            "6/6 [==============================] - 0s 511us/sample - loss: 0.0029\n",
            "Epoch 285/300\n",
            "6/6 [==============================] - 0s 529us/sample - loss: 0.0029\n",
            "Epoch 286/300\n",
            "6/6 [==============================] - 0s 616us/sample - loss: 0.0029\n",
            "Epoch 287/300\n",
            "6/6 [==============================] - 0s 655us/sample - loss: 0.0028\n",
            "Epoch 288/300\n",
            "6/6 [==============================] - 0s 578us/sample - loss: 0.0028\n",
            "Epoch 289/300\n",
            "6/6 [==============================] - 0s 487us/sample - loss: 0.0028\n",
            "Epoch 290/300\n",
            "6/6 [==============================] - 0s 658us/sample - loss: 0.0028\n",
            "Epoch 291/300\n",
            "6/6 [==============================] - 0s 634us/sample - loss: 0.0028\n",
            "Epoch 292/300\n",
            "6/6 [==============================] - 0s 668us/sample - loss: 0.0027\n",
            "Epoch 293/300\n",
            "6/6 [==============================] - 0s 629us/sample - loss: 0.0027\n",
            "Epoch 294/300\n",
            "6/6 [==============================] - 0s 706us/sample - loss: 0.0027\n",
            "Epoch 295/300\n",
            "6/6 [==============================] - 0s 614us/sample - loss: 0.0027\n",
            "Epoch 296/300\n",
            "6/6 [==============================] - 0s 635us/sample - loss: 0.0027\n",
            "Epoch 297/300\n",
            "6/6 [==============================] - 0s 620us/sample - loss: 0.0026\n",
            "Epoch 298/300\n",
            "6/6 [==============================] - 0s 688us/sample - loss: 0.0026\n",
            "Epoch 299/300\n",
            "6/6 [==============================] - 0s 698us/sample - loss: 0.0026\n",
            "Epoch 300/300\n",
            "6/6 [==============================] - 0s 738us/sample - loss: 0.0026\n",
            "[4.073303]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgj2heVdTanP",
        "colab_type": "text"
      },
      "source": [
        "**Exercise two**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE8WZAPhTg1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e73ba68d-3430-4adb-e0ee-7ecf004036ac"
      },
      "source": [
        "import tensorflow as tf\n",
        "from os import path, getcwd, chdir\n",
        "import pandas as pd\n",
        "# DO NOT CHANGE THE LINE BELOW. If you are developing in a local\n",
        "# environment, then grab mnist.npz from the Coursera Jupyter Notebook\n",
        "# and place it inside a local folder and edit the path to that location\n",
        "path = f\"{getcwd()}/../tmp2/mnist.npz\"\n",
        "print(path)\n",
        " "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/../tmp2/mnist.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-0uV147SGVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: train_mnist\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "def train_mnist():\n",
        "\n",
        "\n",
        "\n",
        "    DESIRED_ACCURACY = 0.99\n",
        "    class myCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if(logs.get('acc')>=DESIRED_ACCURACY):\n",
        "                print('\\nDesired Accuracy is met, Stopping training...')\n",
        "                self.model.stop_training = True\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        " \n",
        " \n",
        " \n",
        "    callbacks = myCallback()    \n",
        "    # Please write your code only where you are indicated.\n",
        "    # please do not remove # model fitting inline comments.\n",
        "\n",
        "    # YOUR CODE SHOULD START HERE\n",
        "\n",
        "    # YOUR CODE SHOULD END HERE\n",
        "\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "\n",
        "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "     \n",
        "\n",
        "    # YOUR CODE SHOULD END HERE\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "\n",
        "tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    epochs=10\n",
        "    # model fitting\n",
        "    history = model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])\n",
        "              # YOUR CODE SHOULD END HERE\n",
        "    \n",
        "    # model fitting\n",
        "    return history.epoch, history.history['acc'][-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNUA1UjpWp4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce4f5713-0eff-45b1-fec5-8c246fa25902"
      },
      "source": [
        "train_mnist"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.train_mnist>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTPiX-nmTUN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c3032e8e-bd6d-4b45-cabb-c182bb7f749c"
      },
      "source": [
        "# please do not remove # model fitting inline comments.\n",
        "\n",
        "# YOUR CODE SHOULD START HERE\n",
        "\n",
        "#def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('acc')>0.99):\n",
        "            print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "\n",
        "tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "\n",
        "tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "\n",
        "loss='sparse_categorical_crossentropy',\n",
        "\n",
        "metrics=['accuracy'])\n",
        "\n",
        "# model fitting\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])\n",
        "\n",
        "# model fitting\n",
        "\n",
        "#return history.epoch, history.history['acc'][-1]\n",
        "\n",
        "#model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])\n",
        "\n",
        "# model fitting\n",
        "\n",
        "#return history.epoch, history.history['acc'][-1]\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.2002 - acc: 0.9410\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0820 - acc: 0.9747\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0514 - acc: 0.9836\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0379 - acc: 0.9880\n",
            "Epoch 5/10\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9916\n",
            "Reached 99% accuracy so cancelling training!\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0265 - acc: 0.9916\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}